---
# this is the config that will be loaded as default by main.py
# Please follow the provided structure (this will ensuring all baseline follow
# a similar configuration structure and hence be easy to customise)

algorithm: "FedAvg" # can be "FedAvg" or "FedAvg+KD" or "FedMLB"
model_name: "mit-b0"  # mit-b0, mit-b1, mit-b2, deit_tiny, deit_small
sanitized_dataset: True
optimizer: "adamw"
momentum: 0.0
classifier_hidden_layers: 0
classifier_unit_pl: 0
load_pretrained_weights: True
trainable_feature_extractor: True
trainable_blocks: null
total_clients: 100 # total number of clients
num_rounds: 50
clients_per_round: 10
local_updates: 50
batch_size: 32 # if set to null, it will calculates local_batch_size as round(local_examples * local_epochs / local_updates)
local_epochs: 5 # number of local epochs
lr_client: 3e-4 # client learning rate
exp_decay: 0.998
clipnorm: null
l2_weight_decay: 1e-3
random_seed: 25  # 23 augmentation crop, 24 no augm, 25 also flip
starting_round: 1
restart_from_checkpoint: True # if True, looks for a checkpoint of that config to restart the training
save_checkpoint: True # if True, saves a checkpoint server model at the end of the training
logging_memory_usage: False # if True logs memory and GPU's memory usage (need for psutil and nvidia-smi installed)

client_resources:
  num_cpus: 1.0
  num_gpus: 0.5

dataset_config:
  dataset: "cifar100"
  alpha_dirichlet: -1 # can be 0.3, 0.6

strategy:
  _target_: flwr.server.strategy.FedAvg
  fraction_fit: 0.00001
  fraction_evaluate: 0.0
  min_fit_clients: ${clients_per_round}
  min_evaluate_clients: 0
